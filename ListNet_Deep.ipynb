{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=pd.read_csv('score.csv',header=None)\n",
    "job_feature=pd.read_csv('job_feature.csv',index_col=False)\n",
    "query_feature=pd.read_csv('query_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.columns=['jobno','querystring','y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_feature=job_feature.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feature=query_feature.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_variable(lis,query):\n",
    "    #1 把list中每個元素都找到job_feature並與query join\n",
    "    #2 計算出每個query對應到job的rel分數，若沒有值補0\n",
    "    d = {'jobno': lis, 'querystring': [query for i in lis]}\n",
    "    dat = pd.merge(pd.DataFrame(d), score, how='left', on=['jobno','querystring']).fillna(0)\n",
    "    #3 merge-job feature\n",
    "    dat = pd.merge(pd.DataFrame(dat), job_feature, how='left', on=['jobno'])\n",
    "    #4 merge query feature\n",
    "    dat = pd.merge(pd.DataFrame(dat), query_feature, how='left', on=['querystring'])\n",
    "    \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=get_variable([9999831,10000054,10000112,],'keyword=sales%20assistance&jobsource=n104bank1&ro=0&order=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 31)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.iloc[:,3:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListNet():\n",
    "    def __init__(self,learning_rate=0.001,list_size=3,query_feature_size=31,job_feature_size=0):\n",
    "        tf.reset_default_graph()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.list_size = list_size \n",
    "        self.query_feature_size=query_feature_size\n",
    "        self.job_feature_size=job_feature_size\n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [self.list_size, self.query_feature_size+self.job_feature_size])\n",
    "        self.y = tf.placeholder(tf.float32, [self.list_size, ])\n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # encoder -> 把x encode成 z ->把z encode成mu,sigma \n",
    "        network_weights=self._initialize_weights()\n",
    "        \n",
    "        layer1=tf.add(tf.matmul(self.x,network_weights['weights']['Deep1']),network_weights['b']['Deep1'])\n",
    "        layer1=tf.sigmoid(layer1)\n",
    "        \n",
    "        layer2=tf.add(tf.matmul(layer1,network_weights['weights']['Deep2']),network_weights['b']['Deep2'])\n",
    "        layer2=tf.sigmoid(layer2)\n",
    "        \n",
    "        self.t=tf.add(tf.matmul(layer2,network_weights['weights']['out']),network_weights['b']['out'])\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        all_weights={\n",
    "                \"weights\":{\n",
    "                    \"Deep1\":tf.get_variable('C_W_1', shape=(self.query_feature_size+self.job_feature_size,10), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "                    \"Deep2\":tf.get_variable('C_W_2', shape=(10,5), initializer=tf.contrib.layers.xavier_initializer()) ,\n",
    "                    \"out\":tf.get_variable('out_w', shape=(5,1), initializer=tf.contrib.layers.xavier_initializer()) \n",
    "                },\n",
    "                \"b\":{\n",
    "                    \"Deep1\":tf.get_variable('C_b_1', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),\n",
    "                    \"Deep2\":tf.get_variable('C_b_2', shape=(5), initializer=tf.contrib.layers.xavier_initializer()) ,\n",
    "                    \"out\":tf.get_variable('out_b', shape=(1), initializer=tf.contrib.layers.xavier_initializer()) \n",
    "                }\n",
    "            }\n",
    "        return all_weights\n",
    "    \n",
    "    def _create_loss_optimizer(self):\n",
    "        real_score=tf.divide(tf.exp(self.y),tf.reduce_sum(tf.exp(self.y)))\n",
    "        model_score=tf.divide(tf.exp(self.t),tf.reduce_sum(tf.exp(self.t)))\n",
    "        self.cost = -tf.reduce_sum(tf.multiply(real_score,tf.log(model_score)))\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "    \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "\n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        #opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "    #                                   feed_dict={self.x: X})\n",
    "        opt,cost=self.sess.run((self.optimizer,self.cost),{self.x:X})\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ListNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sess.run(a.optimizer,feed_dict={a.x:np.array(dd.iloc[:,3:]),a.y:np.array(dd['y'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
